{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing a Simple Neural Network Made from Scratch with One Constructed with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this program we will build a neural network from scratch for use on a dataset with only four samples to easily break down the structure and inner workings of the model, and to see how forward propagation and backward propagation can be implemented in practice. Then we will create a similar neural network using Keras for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense \n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some training inputs, X, and a target output, y. X is an array containing four training samples, each with three features. y is an array containing the desired output for each of the four training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input and desired output (target)\n",
    "X = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this neural network we will employ the sigmoid activation function, which will be used directly in forward propagation. The derivative of the sigmoid activation is required for backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "# define the derivative of the sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    return x*(1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In forward propagation the hidden layer activations are given by $\\mathbf{a}^2 = \\sigma(w^2\\mathbf{x} + \\mathbf{b}^2) = \\sigma(\\mathbf{z}^2)$, where $\\mathbf{a}^2$ is the vector of hidden layer activations, $w^2$ is the matrix of weights applied to the inputs, and $\\mathbf{b}^2$ is the vector of biases used to compute the hidden layer activations. Similarly, the output activations are given by $\\mathbf{a}^3 = \\sigma(w^3\\mathbf{a}^2 + \\mathbf{b}^3) = \\sigma(\\mathbf{z}^3)$. In this neural network we will not use biases, i.e., $\\mathbf{b}^2 = \\mathbf{b}^3 = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the feedforward function\n",
    "def feedforward(X, weights2, weights3):\n",
    "    # multiply the inputs by their weights, sum together and pass to the sigmoid function\n",
    "    layer2 = sigmoid(np.dot(X, weights2))\n",
    "    # multiply the hidden layer activation by their weights, sum together and pass to the sigmoid function\n",
    "    output = sigmoid(np.dot(layer2, weights3))\n",
    "    return layer2, output    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we randomly initialise the weights matrices, ensuring that their dimensions permit matrix multiplication with the inputs and hidden layer, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the weights matrices between the input and hidden layers, and the hidden and output layers \n",
    "weights2 = np.random.rand(X.shape[1], 4)\n",
    "weights3 = np.random.rand(4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see exactly how the forward propagation function works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "\n",
      "\n",
      "[[0.34194905 0.24479932 0.04305208 0.6387264 ]\n",
      " [0.8247908  0.28574849 0.61266972 0.8960801 ]\n",
      " [0.70629426 0.92819626 0.89986665 0.93751877]]\n",
      "\n",
      "\n",
      "[[0.70629426 0.92819626 0.89986665 0.93751877]\n",
      " [1.53108506 1.21394475 1.51253637 1.83359887]\n",
      " [1.04824331 1.17299558 0.94291872 1.57624517]\n",
      " [1.87303412 1.45874407 1.55558845 2.47232527]]\n",
      "\n",
      "\n",
      "[[0.66958181 0.7167092  0.7109221  0.71859819]\n",
      " [0.82216502 0.77099618 0.81943679 0.8621899 ]\n",
      " [0.74043742 0.76368605 0.71968885 0.82867209]\n",
      " [0.86680896 0.81134051 0.82571941 0.9221788 ]]\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print('\\n')\n",
    "print(weights2)\n",
    "print('\\n')\n",
    "print(np.dot(X, weights2))\n",
    "print('\\n')\n",
    "print(sigmoid(np.dot(X, weights2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns in the weights matrix contain the weights applied to a particular sample in X. For example, the first column contains the weights applied to the first sample in X (first row), the second column contains the weights applied to the second sample in X (second row), and so on.\n",
    "\n",
    "The matrix np.dot(X, weights1) has as its rows the $z^2_i$ corresponding to the activations $a^2_i$ for each sample. For example, the first row has as its elements $z^2_1$, $z^2_2$, $z^2_3$ and $z^2_4$ for the first sample in X. The second row has as its elements $z^2_1$, $z^2_2$, $z^2_3$ and $z^2_4$ for the second sample in X, and so on.\n",
    "\n",
    "sigmoid(np.dot(X, weights1) has as its rows the hidden layers activations $a^2_1$, $a^2_2$, $a^2_3$, and $a^2_4$ for each training sample. For example, the first row contains the four hidden layer activations corresponding to the first training sample, the second row contains the four hidden layer activations corresponding to the second training sample in X etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define the Mean Squared Error (MSE) cost function as $C = \\frac{1}{2}[y - \\sigma(z)]^2$.\n",
    "\n",
    "In order to undertake gradient descent, we need to calculate the partial derivatives of $C$ with respect to $w^2$ and $w^3$:\n",
    "$$\\frac{\\partial C}{\\partial w^3} = \\frac{\\partial C}{\\partial \\sigma}\\frac{\\partial \\sigma}{\\partial \\mathbf{z}^3}\\frac{\\partial \\mathbf{z}^3}{\\partial w^3} = [y - \\sigma(\\mathbf{z}^3)]\\sigma'(\\mathbf{z}^3)\\mathbf{a}^2$$\n",
    "\n",
    "Now we know $\\partial C/\\partial w^3$ we can compute $\\partial C/\\partial w^2$:\n",
    "$$\\frac{\\partial C}{\\partial w^2} = \\frac{\\partial C}{\\partial w^3}\\frac{\\partial w^3}{\\partial w^2} = [y -\\sigma(\\mathbf{z}^3)]\\sigma'(\\mathbf{z}^3)\\mathbf{a}^2\\frac{\\partial w^3}{\\partial w^2}$$\n",
    "$$\\frac{\\partial w^3}{\\partial w^2} = \\frac{\\partial w^3}{\\partial \\mathbf{z}^3}\\frac{\\partial \\mathbf{z}^3}{\\partial \\mathbf{z}^2}\\frac{\\partial \\mathbf{z}^2}{\\partial w^2} = \\frac{1}{\\mathbf{a}^2}w^3\\sigma'(\\mathbf{z}^2)\\mathbf{a}^1$$\n",
    "Therefore, $$\\frac{\\partial C}{\\partial w^2} = \\mathbf{a}^1[y -\\sigma(\\mathbf{z}^3)]\\sigma'(\\mathbf{z}^3)w^3\\sigma'(\\mathbf{z}^2)$$\n",
    "\n",
    "This process of finding the partial derivatives of $C$ is called backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(X, y, layer1, output, weights1, weights2):\n",
    "    # derivative of the second weights matrix\n",
    "    d_weights2 = np.dot(layer1.T, (y - output)*sigmoid_derivative(output))\n",
    "    # derivative of the first weights matrix\n",
    "    d_weights1 = np.dot(X.T, (np.dot((y - output)*sigmoid_derivative(output), weights2.T)\n",
    "                                           *sigmoid_derivative(layer1)))\n",
    "    return d_weights1, d_weights2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have calculated the partial derivatives $\\partial C/\\partial w^2$ and $\\partial C/\\partial w^3$ we simultaneously update $w^2$ and $w^3$ according to:\n",
    "$$w^i \\rightarrow w^i - \\eta\\frac{\\partial C}{\\partial w^i},\\;\\;\\;\\;\\;\\;\\mbox{for}\\;\\; i = 2, 3$$\n",
    "where $\\eta$ is the learning rate (below we will use $\\eta = 1$).\n",
    "\n",
    "We begin training the neural network by forward propagating the inputs through the network to generate the output. Then we backpropagate to find the partial derivatives of the cost function and use these to update the weights. This process is repeated many times as the weights are optimised via gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0130081 ]\n",
      " [0.96259263]\n",
      " [0.96336681]\n",
      " [0.04577575]]\n"
     ]
    }
   ],
   "source": [
    "# run feedforward and backpropagation over 2000 iterations\n",
    "for i in range(2000):\n",
    "    layer2, output = feedforward(X, weights2, weights3)\n",
    "    d_weights2, d_weights3 = backpropagation(X, y, layer2, output, weights2, weights3)\n",
    "    \n",
    "    # gradient descent: update the weights matrices \n",
    "    weights2 += d_weights2\n",
    "    weights3 += d_weights3\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily create a similar neural network using Keras. To build a linear set of layers, we initiate a Sequential model and add layers to it. The first layer we add (hidden layer) will have four neurons and will provide weights for the three input features of the samples. The second added layer is the output layer of a single neuron. A dense layer is a fully-connected layer, i.e., all the neurons in a layer are connected to those in the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 4)                 16        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 21\n",
      "Trainable params: 21\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# construct hidden layer of four neurons\n",
    "model.add(Dense(units = 4, activation = 'sigmoid', input_dim = 3))\n",
    "# construct output layer\n",
    "model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model summary shows that the first layer contains 16 parameters for training; for the $j$-th neuron in the hidden layer we have $$z^2_j = w^2_{j1}a^1_1 + w^2_{j2}a^1_2 + w^2_{j3}a^1_3 + b^2_j,\\;\\;\\;\\;\\;\\;\\mbox{for}\\;\\; j = 1,\\;2,\\;3,\\;4$$\n",
    "The output layer contains five parameters:\n",
    "$$z^3_1 = w^3_{11}a^2_1 + w^3_{12}a^3_2 + w^3_{13}a^2_3 + w^3_{14}a^3_4 + b^2_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will utilise the SGD optimiser which employs the gradient descent algorithm to train the model with a learning rate of one. This optimiser works well for shallow neural networks. We pass the optimiser to the compile method along with the cost function we wish to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04577421]\n",
      " [0.95596975]\n",
      " [0.95841074]\n",
      " [0.03405814]]\n"
     ]
    }
   ],
   "source": [
    "sgd = optimizers.SGD(lr = 1)\n",
    "model.compile(loss = 'mean_squared_error', optimizer = sgd)\n",
    "model.fit(X, y, epochs = 2000, verbose = False)\n",
    "print(model.predict(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
